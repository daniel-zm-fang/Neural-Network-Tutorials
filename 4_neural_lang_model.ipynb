{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on \"A Neural Probabilistic Language Model\" by Bengio et al. (2003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/names.txt', 'r') as f:\n",
    "    words = [word.strip().lower() + '.' for word in f.readlines()]\n",
    "\n",
    "train, valid, test = 0.8, 0.1, 0.1\n",
    "train_words = words[:int(len(words) * train)]\n",
    "valid_words = words[int(len(words) * train):int(len(words) * (train + valid))]\n",
    "test_words = words[int(len(words) * (train + valid)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataSet(Dataset):\n",
    "    def __init__(self, words, blck_size):\n",
    "        self.words = words\n",
    "        self.blck_size = blck_size\n",
    "\n",
    "        # Create a dictionary that maps char to integers, and vice versa\n",
    "        self.chars = sorted(list(set(''.join(self.words))))\n",
    "        self.char_to_int = {c: i + 1 for i, c in enumerate(self.chars)}\n",
    "        self.char_to_int['.'] = 0\n",
    "        self.int_to_char = {i: c for c, i in self.char_to_int.items()}\n",
    "\n",
    "        X, y = [], []\n",
    "        for w in self.words:\n",
    "            context = [0] * blck_size\n",
    "            for i in range(len(w)):\n",
    "                idx = self.char_to_int[w[i]]\n",
    "                X.append(context)\n",
    "                y.append(idx)\n",
    "                context = context[1:] + [idx]\n",
    "\n",
    "        self.X = torch.tensor(X)\n",
    "        self.y = torch.tensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "blck_size = 3\n",
    "train_ds = WordDataSet(train_words, blck_size)\n",
    "valid_ds = WordDataSet(valid_words, blck_size)\n",
    "test_ds = WordDataSet(test_words, blck_size)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=64, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNLM(torch.nn.Module):\n",
    "    # Neural Network Language Model\n",
    "    def __init__(self, emb_size, hidden_size, vocab_size, blck_size):\n",
    "        super(NNLM, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.blck_size = blck_size\n",
    "\n",
    "        self.emb = torch.nn.Parameter(torch.randn(vocab_size, emb_size))\n",
    "        self.W1 = torch.nn.Parameter(torch.randn(emb_size * blck_size, hidden_size))\n",
    "        self.b1 = torch.nn.Parameter(torch.randn(hidden_size))\n",
    "        self.W2 = torch.nn.Parameter(torch.randn(hidden_size, vocab_size))\n",
    "        self.b2 = torch.nn.Parameter(torch.randn(vocab_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb[x]\n",
    "        x = x.view(-1, self.emb_size * self.blck_size)\n",
    "        x = torch.tanh(torch.matmul(x, self.W1) + self.b1)\n",
    "        x = torch.matmul(x, self.W2) + self.b2\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding size > 32 doesn't make sense, because the alphabet size is only 26."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 25924\n",
      "Epoch: 1/100 | Train loss: 7.0049 | Valid loss: 5.2712\n",
      "Epoch: 2/100 | Train loss: 3.7116 | Valid loss: 4.2324\n",
      "Epoch: 3/100 | Train loss: 3.1267 | Valid loss: 3.7745\n",
      "Epoch: 4/100 | Train loss: 2.8431 | Valid loss: 3.4917\n",
      "Epoch: 5/100 | Train loss: 2.6661 | Valid loss: 3.2986\n",
      "Epoch: 6/100 | Train loss: 2.5469 | Valid loss: 3.1518\n",
      "Epoch: 7/100 | Train loss: 2.4584 | Valid loss: 3.0523\n",
      "Epoch: 8/100 | Train loss: 2.3907 | Valid loss: 2.9591\n",
      "Epoch: 9/100 | Train loss: 2.3379 | Valid loss: 2.8861\n",
      "Epoch: 10/100 | Train loss: 2.2937 | Valid loss: 2.8167\n",
      "Epoch: 11/100 | Train loss: 2.2584 | Valid loss: 2.7544\n",
      "Epoch: 12/100 | Train loss: 2.2287 | Valid loss: 2.7148\n",
      "Epoch: 13/100 | Train loss: 2.2032 | Valid loss: 2.6903\n",
      "Epoch: 14/100 | Train loss: 2.1820 | Valid loss: 2.6507\n",
      "Epoch: 15/100 | Train loss: 2.1633 | Valid loss: 2.6248\n",
      "Epoch: 16/100 | Train loss: 2.1480 | Valid loss: 2.6088\n",
      "Epoch: 17/100 | Train loss: 2.1359 | Valid loss: 2.5732\n",
      "Epoch: 18/100 | Train loss: 2.1241 | Valid loss: 2.5571\n",
      "Epoch: 19/100 | Train loss: 2.1138 | Valid loss: 2.5398\n",
      "Epoch: 20/100 | Train loss: 2.1051 | Valid loss: 2.5157\n",
      "Epoch: 21/100 | Train loss: 2.0968 | Valid loss: 2.5115\n",
      "Epoch: 22/100 | Train loss: 2.0907 | Valid loss: 2.5081\n",
      "Epoch: 23/100 | Train loss: 2.0839 | Valid loss: 2.4906\n",
      "Epoch: 24/100 | Train loss: 2.0787 | Valid loss: 2.4827\n",
      "Epoch: 25/100 | Train loss: 2.0740 | Valid loss: 2.4657\n",
      "Epoch: 26/100 | Train loss: 2.0702 | Valid loss: 2.4703\n",
      "Epoch: 27/100 | Train loss: 2.0651 | Valid loss: 2.4599\n",
      "Epoch: 28/100 | Train loss: 2.0621 | Valid loss: 2.4435\n",
      "Epoch: 29/100 | Train loss: 2.0588 | Valid loss: 2.4456\n",
      "Epoch: 30/100 | Train loss: 2.0554 | Valid loss: 2.4361\n",
      "Epoch: 31/100 | Train loss: 2.0526 | Valid loss: 2.4316\n",
      "Epoch: 32/100 | Train loss: 2.0504 | Valid loss: 2.4366\n",
      "Epoch: 33/100 | Train loss: 2.0476 | Valid loss: 2.4398\n",
      "Epoch: 34/100 | Train loss: 2.0454 | Valid loss: 2.4283\n",
      "Epoch: 35/100 | Train loss: 2.0429 | Valid loss: 2.4132\n",
      "Epoch: 36/100 | Train loss: 2.0420 | Valid loss: 2.4280\n",
      "Epoch: 37/100 | Train loss: 2.0386 | Valid loss: 2.4241\n",
      "Epoch: 38/100 | Train loss: 2.0375 | Valid loss: 2.4222\n",
      "Epoch: 39/100 | Train loss: 2.0360 | Valid loss: 2.4219\n",
      "Epoch: 40/100 | Train loss: 2.0343 | Valid loss: 2.4182\n",
      "Epoch: 41/100 | Train loss: 2.0324 | Valid loss: 2.4207\n",
      "Epoch: 42/100 | Train loss: 2.0304 | Valid loss: 2.4131\n",
      "Epoch: 43/100 | Train loss: 2.0301 | Valid loss: 2.4020\n",
      "Epoch: 44/100 | Train loss: 2.0291 | Valid loss: 2.4033\n",
      "Epoch: 45/100 | Train loss: 2.0269 | Valid loss: 2.4192\n",
      "Epoch: 46/100 | Train loss: 2.0253 | Valid loss: 2.4035\n",
      "Epoch: 47/100 | Train loss: 2.0244 | Valid loss: 2.4103\n",
      "Epoch: 48/100 | Train loss: 2.0230 | Valid loss: 2.4073\n",
      "Epoch: 49/100 | Train loss: 2.0219 | Valid loss: 2.4055\n",
      "Epoch: 50/100 | Train loss: 2.0211 | Valid loss: 2.4013\n",
      "Epoch: 51/100 | Train loss: 2.0197 | Valid loss: 2.4050\n",
      "Epoch: 52/100 | Train loss: 2.0189 | Valid loss: 2.3942\n",
      "Epoch: 53/100 | Train loss: 2.0183 | Valid loss: 2.3963\n",
      "Epoch: 54/100 | Train loss: 2.0168 | Valid loss: 2.4044\n",
      "Epoch: 55/100 | Train loss: 2.0159 | Valid loss: 2.4011\n",
      "Epoch: 56/100 | Train loss: 2.0152 | Valid loss: 2.4095\n",
      "Epoch: 57/100 | Train loss: 2.0143 | Valid loss: 2.3954\n",
      "Epoch: 58/100 | Train loss: 2.0135 | Valid loss: 2.3999\n",
      "Epoch: 59/100 | Train loss: 2.0131 | Valid loss: 2.4000\n",
      "Epoch: 60/100 | Train loss: 2.0114 | Valid loss: 2.3959\n",
      "Epoch: 61/100 | Train loss: 2.0119 | Valid loss: 2.3936\n",
      "Epoch: 62/100 | Train loss: 2.0101 | Valid loss: 2.4018\n",
      "Epoch: 63/100 | Train loss: 2.0098 | Valid loss: 2.3896\n",
      "Epoch: 64/100 | Train loss: 2.0089 | Valid loss: 2.3956\n",
      "Epoch: 65/100 | Train loss: 2.0082 | Valid loss: 2.3874\n",
      "Epoch: 66/100 | Train loss: 2.0079 | Valid loss: 2.3973\n",
      "Epoch: 67/100 | Train loss: 2.0068 | Valid loss: 2.3880\n",
      "Epoch: 68/100 | Train loss: 2.0066 | Valid loss: 2.3923\n",
      "Epoch: 69/100 | Train loss: 2.0061 | Valid loss: 2.3896\n",
      "Epoch: 70/100 | Train loss: 2.0057 | Valid loss: 2.4004\n",
      "Epoch: 71/100 | Train loss: 2.0051 | Valid loss: 2.3971\n",
      "Epoch: 72/100 | Train loss: 2.0036 | Valid loss: 2.3981\n",
      "Epoch: 73/100 | Train loss: 2.0043 | Valid loss: 2.3836\n",
      "Epoch: 74/100 | Train loss: 2.0032 | Valid loss: 2.3836\n",
      "Epoch: 75/100 | Train loss: 2.0034 | Valid loss: 2.4014\n",
      "Epoch: 76/100 | Train loss: 2.0030 | Valid loss: 2.3870\n",
      "Epoch: 77/100 | Train loss: 2.0022 | Valid loss: 2.4020\n",
      "Epoch: 78/100 | Train loss: 2.0014 | Valid loss: 2.3901\n",
      "Epoch: 79/100 | Train loss: 2.0014 | Valid loss: 2.3836\n",
      "Epoch: 80/100 | Train loss: 2.0002 | Valid loss: 2.3865\n",
      "Epoch: 81/100 | Train loss: 2.0003 | Valid loss: 2.3845\n",
      "Epoch: 82/100 | Train loss: 1.9998 | Valid loss: 2.3865\n",
      "Epoch: 83/100 | Train loss: 2.0002 | Valid loss: 2.3828\n",
      "Epoch: 84/100 | Train loss: 1.9989 | Valid loss: 2.3996\n",
      "Epoch: 85/100 | Train loss: 1.9991 | Valid loss: 2.3883\n",
      "Epoch: 86/100 | Train loss: 1.9983 | Valid loss: 2.3850\n",
      "Epoch: 87/100 | Train loss: 1.9976 | Valid loss: 2.3827\n",
      "Epoch: 88/100 | Train loss: 1.9970 | Valid loss: 2.3898\n",
      "Epoch: 89/100 | Train loss: 1.9971 | Valid loss: 2.3801\n",
      "Epoch: 90/100 | Train loss: 1.9970 | Valid loss: 2.3906\n",
      "Epoch: 91/100 | Train loss: 1.9957 | Valid loss: 2.3918\n",
      "Epoch: 92/100 | Train loss: 1.9957 | Valid loss: 2.3808\n",
      "Epoch: 93/100 | Train loss: 1.9950 | Valid loss: 2.3908\n",
      "Epoch: 94/100 | Train loss: 1.9951 | Valid loss: 2.3945\n",
      "Epoch: 95/100 | Train loss: 1.9948 | Valid loss: 2.3861\n",
      "Epoch: 96/100 | Train loss: 1.9941 | Valid loss: 2.3845\n",
      "Epoch: 97/100 | Train loss: 1.9938 | Valid loss: 2.3798\n",
      "Epoch: 98/100 | Train loss: 1.9931 | Valid loss: 2.3807\n",
      "Epoch: 99/100 | Train loss: 1.9928 | Valid loss: 2.3825\n",
      "Epoch: 100/100 | Train loss: 1.9925 | Valid loss: 2.3922\n"
     ]
    }
   ],
   "source": [
    "model = NNLM(32, 200, len(train_ds.chars) + 1, blck_size)\n",
    "print(f'Number of parameters: {sum([p.numel() for p in model.parameters()])}')\n",
    "\n",
    "num_epochs = 100\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_loss, valid_loss = [], []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for x, y in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss.append(running_loss / len(train_dl))\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    for x, y in valid_dl:\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        running_loss += loss.item()\n",
    "    valid_loss.append(running_loss / len(valid_dl))\n",
    "\n",
    "    print(f'Epoch: {epoch + 1}/{num_epochs} | Train loss: {train_loss[-1]:.4f} | Valid loss: {valid_loss[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ellenzi\n",
      "aysha\n",
      "belles\n",
      "ronix\n",
      "larenley\n",
      "gracerem\n",
      "cora\n",
      "jiai\n",
      "haden\n",
      "mace\n"
     ]
    }
   ],
   "source": [
    "# sampling\n",
    "max_length = 20\n",
    "num_words = 10\n",
    "\n",
    "model.eval()\n",
    "for _ in range(num_words):\n",
    "    context = [0] * blck_size\n",
    "    generated_word = []\n",
    "    for _ in range(max_length):\n",
    "        x = torch.tensor([context])\n",
    "        y_hat = model(x)\n",
    "        # output has shape (1, vocab_size), squeeze to (vocab_size)\n",
    "        probs = F.softmax(y_hat, dim=1).squeeze()\n",
    "        sampled_char_idx = torch.multinomial(probs, 1).item()\n",
    "        if sampled_char_idx == 0:\n",
    "            break\n",
    "        generated_word.append(train_ds.int_to_char[sampled_char_idx])\n",
    "        context = context[1:] + [sampled_char_idx]\n",
    "    print(''.join(generated_word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
